{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e23079",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "> Reinforcement Learning Programming- CSCN 8020 <br>\n",
    "> Name: Tai Siang Huang <br>\n",
    "> ID: 9006413 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72505b",
   "metadata": {},
   "source": [
    "## **Problem 1 [10]**\n",
    "Pick-and-Place Robot: Consider using reinforcement learning to control the motion of a robot arm in a repetitive pick-and-place task.\n",
    "Design the reinforcement learning problem as an MDP, define states, actions, rewards with reasoning.\n",
    "\n",
    "### Assuming we are controlling a 3-DOF robotic arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc442fe1",
   "metadata": {},
   "source": [
    "### Sates\n",
    "1. **Joint Angles:** `[θ₁, θ₂, θ₃]` <br>\n",
    "   - Reason: Agent can learn the relationship between itself joint anglesa and space position.<br>\n",
    "2. **Joint Angles' speed:** `[θ₁_v, θ₂_v, θ₃_v]`<br>\n",
    "   - Reason: Agent can learn for controlling smooth and fast movement.<br>\n",
    "3. **End-effector Position:** `[x, y, z]`<br>\n",
    "   - Reason: Agent can determine the space's position itself.<br>\n",
    "4. **Graspper State:** `[gripper_state]`<br>\n",
    "   - Reason: Agent can know the gripper status.<br>\n",
    "5. **Object Position:** `[x_obj, y_obj, z_obj]`<br>\n",
    "   - Reason: Agent can learn where the object is and where to catch <br>\n",
    "6. **Target Position:** `[x_target, y_target, z_target]`<br>\n",
    "   - Reason: Agent can learn where the target is and where to place<br>\n",
    "7. **Step Count:** `[step_count]`<br>\n",
    "   - Reason: Agent can learn how to complete the task in the shortest time<br>\n",
    "\n",
    "**Total Sates:** `s = [θ₁, θ₂, θ₃, θ₁_v, θ₂_v, θ₃_v, x, y, z, gripper_state, x_obj, y_obj, z_obj, x_target, y_target, z_target, step_count]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e87b6c",
   "metadata": {},
   "source": [
    "### Actions\n",
    "- `a = [Δx, Δy, Δz, g]`\n",
    "- **End-effector Control:** Learning policies is more intuitive, \"move towards an object\" rather than \"adjust a joint\"\n",
    "- **Gripper Action:** Gripper behavior: 1 = off, 0 = on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272290",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "1. **Close to object:** Encourage the agent to move closer to the object.\n",
    "2. **Successfully grasp the object:** A large reward will be given when the object is successfully grasped.\n",
    "3. **Close to the target location:** When the object is grasped, encourage approach to the target location.\n",
    "4. **Successfully drop the object in the target area:** Big rewards for completing tasks.\n",
    "5. **Additional penalties:** Encourage completing tasks faster by penalizing every step, penalty for placing or dropping in the wrong place.\n",
    "   \n",
    "**Note:** Moving tasks use the linear positive to give reward, can control robot smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ae18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Reward Function\n",
    "# ------------------------------\n",
    "def compute_reward(state):\n",
    "    ee_pos = np.array(state[\"end_effector_pos\"])\n",
    "    obj_pos = np.array(state[\"object_pos\"])\n",
    "    tgt_pos = np.array(state[\"target_pos\"])\n",
    "    \n",
    "    # Tunable parameters\n",
    "    grasp_threshold = 0.05      # If EE-object distance < this, consider grasped\n",
    "    place_threshold = 0.05      # If object-target distance < this, consider placed\n",
    "    drop_threshold = 0.1        # If object far from both EE and target, consider dropped\n",
    "    max_distance = 1.0          # For distance normalization\n",
    "\n",
    "    # Distance calculations\n",
    "    dist_to_object = np.linalg.norm(ee_pos - obj_pos)\n",
    "    dist_to_target = np.linalg.norm(obj_pos - tgt_pos)\n",
    "    \n",
    "    # State inference\n",
    "    grasped = state[\"gripper_state\"] == 1 and dist_to_object < grasp_threshold\n",
    "    placed_correctly = state[\"gripper_state\"] == 0 and dist_to_target < place_threshold\n",
    "    dropped_object_elsewhere = (\n",
    "        state[\"gripper_state\"] == 0 and\n",
    "        dist_to_object > drop_threshold and\n",
    "        dist_to_target > drop_threshold\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Reward calculation (linear distance + task stages)\n",
    "    r1 = 1 - (dist_to_object / max_distance)        # Closer to object\n",
    "    r1 = max(r1, 0)\n",
    "    \n",
    "    r2 = 10 if grasped else 0                       # Successful grasp\n",
    "    \n",
    "    r3 = 0\n",
    "    if grasped:\n",
    "        ee_to_target = np.linalg.norm(ee_pos - tgt_pos)\n",
    "        r3 = 1 - (ee_to_target / max_distance)      # While grasping, closer to target\n",
    "        r3 = max(r3, 0)\n",
    "    \n",
    "    r4 = 20 if placed_correctly else 0              # Successful placement\n",
    "    r5 = -0.1                                       # Step penalty\n",
    "    r6 = -10 if dropped_object_elsewhere else 0     # Drop penalty\n",
    "\n",
    "    total_reward = r1 + r2 + r3 + r4 + r5 + r6\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f11ffe",
   "metadata": {},
   "source": [
    "## Problem 2 [20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400f1b4",
   "metadata": {},
   "source": [
    "### Iteration 1:\n",
    "#### 1. Show the initial value function (V) for each state.\n",
    "|   |   |\n",
    "|---|---|\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93930dcd",
   "metadata": {},
   "source": [
    "####  2. Perform value function updates.\n",
    "Bellman Equation: $ V(s)=R(s)+γ⋅V(s')$\n",
    "<br>\n",
    "Due to **Initial Policy (π)** always go **up**, and default γ = 1:\n",
    "- V(s1) = 5 + 0 = 5\n",
    "- V(s2) = 10 + 0 = 10\n",
    "- V(s3) = 1 + V(s1) = 1 + 5 = 6\n",
    "- V(s4) = 2 + V(s2) = 2 + 10 = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27e9e7",
   "metadata": {},
   "source": [
    "####  3. Show the updated value function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eba34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_bd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
