{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e23079",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "> Reinforcement Learning Programming- CSCN 8020 <br>\n",
    "> Name: Tai Siang Huang <br>\n",
    "> ID: 9006413 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72505b",
   "metadata": {},
   "source": [
    "## **Problem 1 [10]**\n",
    "Pick-and-Place Robot: Consider using reinforcement learning to control the motion of a robot arm in a repetitive pick-and-place task.\n",
    "Design the reinforcement learning problem as an MDP, define states, actions, rewards with reasoning.\n",
    "\n",
    "### Assuming we are controlling a 3-DOF robotic arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc442fe1",
   "metadata": {},
   "source": [
    "### Sates\n",
    "1. **Joint Angles:** `[θ₁, θ₂, θ₃]` <br>\n",
    "   - Reason: Agent can learn the relationship between itself joint anglesa and space position.<br>\n",
    "2. **Joint Angles' speed:** `[θ₁_v, θ₂_v, θ₃_v]`<br>\n",
    "   - Reason: Agent can learn for controlling smooth and fast movement.<br>\n",
    "3. **End-effector Position:** `[x, y, z]`<br>\n",
    "   - Reason: Agent can determine the space's position itself.<br>\n",
    "4. **Graspper State:** `[gripper_state]`<br>\n",
    "   - Reason: Agent can know the gripper status.<br>\n",
    "5. **Object Position:** `[x_obj, y_obj, z_obj]`<br>\n",
    "   - Reason: Agent can learn where the object is and where to catch <br>\n",
    "6. **Target Position:** `[x_target, y_target, z_target]`<br>\n",
    "   - Reason: Agent can learn where the target is and where to place<br>\n",
    "7. **Step Count:** `[step_count]`<br>\n",
    "   - Reason: Agent can learn how to complete the task in the shortest time<br>\n",
    "\n",
    "**Total Sates:** `s = [θ₁, θ₂, θ₃, θ₁_v, θ₂_v, θ₃_v, x, y, z, gripper_state, x_obj, y_obj, z_obj, x_target, y_target, z_target, step_count]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e87b6c",
   "metadata": {},
   "source": [
    "### Actions\n",
    "- `a = [Δx, Δy, Δz, g]`\n",
    "- **End-effector Control:** Learning policies is more intuitive, \"move towards an object\" rather than \"adjust a joint\"\n",
    "- **Gripper Action:** Gripper behavior: 1 = off, 0 = on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272290",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "1. **Close to object:** Encourage the agent to move closer to the object.\n",
    "2. **Successfully grasp the object:** A large reward will be given when the object is successfully grasped.\n",
    "3. **Close to the target location:** When the object is grasped, encourage approach to the target location.\n",
    "4. **Successfully drop the object in the target area:** Big rewards for completing tasks.\n",
    "5. **Additional penalties:** Encourage completing tasks faster by penalizing every step, penalty for placing or dropping in the wrong place.\n",
    "   \n",
    "**Note:** Moving tasks use the linear positive to give reward, can control robot smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ae18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Reward Function\n",
    "# ------------------------------\n",
    "def compute_reward(state):\n",
    "    ee_pos = np.array(state[\"end_effector_pos\"])\n",
    "    obj_pos = np.array(state[\"object_pos\"])\n",
    "    tgt_pos = np.array(state[\"target_pos\"])\n",
    "    \n",
    "    # Tunable parameters\n",
    "    grasp_threshold = 0.05      # If EE-object distance < this, consider grasped\n",
    "    place_threshold = 0.05      # If object-target distance < this, consider placed\n",
    "    drop_threshold = 0.1        # If object far from both EE and target, consider dropped\n",
    "    max_distance = 1.0          # For distance normalization\n",
    "\n",
    "    # Distance calculations\n",
    "    dist_to_object = np.linalg.norm(ee_pos - obj_pos)\n",
    "    dist_to_target = np.linalg.norm(obj_pos - tgt_pos)\n",
    "    \n",
    "    # State inference\n",
    "    grasped = state[\"gripper_state\"] == 1 and dist_to_object < grasp_threshold\n",
    "    placed_correctly = state[\"gripper_state\"] == 0 and dist_to_target < place_threshold\n",
    "    dropped_object_elsewhere = (\n",
    "        state[\"gripper_state\"] == 0 and\n",
    "        dist_to_object > drop_threshold and\n",
    "        dist_to_target > drop_threshold\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Reward calculation (linear distance + task stages)\n",
    "    r1 = 1 - (dist_to_object / max_distance)        # Closer to object\n",
    "    r1 = max(r1, 0)\n",
    "    \n",
    "    r2 = 10 if grasped else 0                       # Successful grasp\n",
    "    \n",
    "    r3 = 0\n",
    "    if grasped:\n",
    "        ee_to_target = np.linalg.norm(ee_pos - tgt_pos)\n",
    "        r3 = 1 - (ee_to_target / max_distance)      # While grasping, closer to target\n",
    "        r3 = max(r3, 0)\n",
    "    \n",
    "    r4 = 20 if placed_correctly else 0              # Successful placement\n",
    "    r5 = -0.1                                       # Step penalty\n",
    "    r6 = -10 if dropped_object_elsewhere else 0     # Drop penalty\n",
    "\n",
    "    total_reward = r1 + r2 + r3 + r4 + r5 + r6\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f11ffe",
   "metadata": {},
   "source": [
    "## Problem 2 [20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400f1b4",
   "metadata": {},
   "source": [
    "### Iteration 1:\n",
    "#### 1. Show the initial value function (V) for each state.\n",
    "|   |   |\n",
    "|---|---|\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93930dcd",
   "metadata": {},
   "source": [
    "####  2. Perform value function updates.\n",
    "Bellman Equation: $ V(s)=R(s)+γ⋅V(s')$\n",
    "<br>\n",
    "Due to **Initial Policy (π)** always go **up**, and default γ = 1:\n",
    "- V(s1) = R(s₁) + V₀(s₁) = 5 + 0 = 5\n",
    "- V(s2) = R(s₂) + V₀(s₂) = 10 + 0 = 10\n",
    "- V(s3) = R(s₃) + V₀(s₁) = 1 + 0 = 1\n",
    "- V(s4) = R(s₄) + V₀(s₂) = 2 + 0 = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27e9e7",
   "metadata": {},
   "source": [
    "####  3. Show the updated value function.\n",
    "|   |   |\n",
    "|---|---|\n",
    "| 5 | 10 |\n",
    "| 1 | 2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6efb3",
   "metadata": {},
   "source": [
    "### Iteration 2:\n",
    "#### Show the value function (V) after the second iteration.\n",
    "- V₂(s₁) = R(s₁) + V₁(s₁) = 5 + 5 = 10\n",
    "- V₂(s₂) = R(s₂) + V₁(s₂) = 10 + 10 = 20\n",
    "- V₂(s₃) = R(s₃) + V₁(s₁) = 1 + 5 = 6\n",
    "- V₂(s₄) = R(s₄) + V₁(s₂) = 2 + 10 = 12\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "| 10 | 20 |\n",
    "| 6 | 12 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad691b",
   "metadata": {},
   "source": [
    "## Problem 3 [35]\n",
    "### Task1: Update MDP Code\n",
    "#### 1. Update the reward function to be a list of reward based on whether the state is terminal, grey, or a regular state.\n",
    "- updated the `value_iteration.py` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ENV_SIZE = 5\n",
    "\n",
    "\n",
    "class GridWorld():\n",
    "\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((env_size, env_size))\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (1, 0), (1, 0), (-1, 0)]  # Right, Down, Down, Up\n",
    "        self.action_description = [\"Right\", \"Down\", \"Down\", \"Up\"]\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "\n",
    "        # Updated reward function (Task 1)\n",
    "        self.rewards = {\n",
    "            (4, 4): 10,  # Terminal state\n",
    "            (2, 2): -5,  # Grey states\n",
    "            (3, 0): -5,\n",
    "            (0, 4): -5\n",
    "        }\n",
    "        self.default_reward = -1  # Regular states\n",
    "\n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "        self.pi_str = [[\"\" for _ in range(env_size)]\n",
    "                       for _ in range(env_size)]  # Initialize pi_str\n",
    "\n",
    "    '''@brief Returns the reward for a given state\n",
    "    '''\n",
    "    def get_reward(self, i, j):\n",
    "        return self.rewards.get((i, j), self.default_reward)\n",
    "\n",
    "    '''@brief Checks if the change in V is less than preset threshold\n",
    "    '''\n",
    "    def is_done(self, new_V):\n",
    "        delta = abs(self.V - new_V)\n",
    "        max_delta = delta.max()\n",
    "        return max_delta <= 0.0001  # Theta threshold for convergence\n",
    "\n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self.terminal_state\n",
    "\n",
    "    '''\n",
    "    @brief Overwrites the current state-value function with a new one\n",
    "    '''\n",
    "    def update_value_function(self, V):\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    '''\n",
    "    @brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def get_policy(self):\n",
    "        return self.pi_greedy\n",
    "\n",
    "    '''@brief Prints the policy using the action descriptions\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(\n",
    "                \" \".join([f\"{action:7}\" if action else \"term   \" for action in row]))\n",
    "\n",
    "    '''@brief Calculate the maximum value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # Find the maximum value for the current state using Bellman's equation\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        for action_index in range(len(self.actions)):\n",
    "            next_i, next_j = self.step(action_index, i, j)\n",
    "            if self.is_valid_state(next_i, next_j):\n",
    "                reward = self.get_reward(i, j)  # Reward based on current state\n",
    "                value = reward + self.gamma * self.V[next_i, next_j]\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = action_index\n",
    "                    best_actions_str = self.action_description[action_index]\n",
    "                elif value == max_value:\n",
    "                    best_actions_str += \"|\" + \\\n",
    "                        self.action_description[action_index]\n",
    "        return max_value, best_action, best_actions_str\n",
    "\n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # Deterministic transitions: P(s'|s) = 1.0 for valid moves, else stay\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j  # Stay in place if invalid\n",
    "        return next_i, next_j\n",
    "\n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "\n",
    "    def update_greedy_policy(self):\n",
    "        for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if self.is_terminal_state(i, j):\n",
    "                    self.pi_greedy[i, j] = 0  # No action in terminal state\n",
    "                    self.pi_str[i][j] = \"\"\n",
    "                    continue\n",
    "                _, self.pi_greedy[i, j], action_str = self.calculate_max_value(\n",
    "                    i, j)\n",
    "                self.pi_str[i][j] = action_str\n",
    "\n",
    "\n",
    "# Perform Value Iteration\n",
    "gridworld = GridWorld(ENV_SIZE)\n",
    "num_iterations = 1000\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    new_V = np.copy(gridworld.get_value_function())\n",
    "    for i in range(ENV_SIZE):\n",
    "        for j in range(ENV_SIZE):\n",
    "            if not gridworld.is_terminal_state(i, j):\n",
    "                new_V[i, j], _, _ = gridworld.calculate_max_value(i, j)\n",
    "    gridworld.update_value_function(new_V)\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal Value Function:\")\n",
    "print(gridworld.get_value_function())\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "gridworld.update_greedy_policy()\n",
    "gridworld.print_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7f365",
   "metadata": {},
   "source": [
    "####  2. Run the existing code developed in class and obtain the optimal state-values and optimal policy. Provide a figures of the gridworld with the obtained V∗ and π∗ (You can manually create a table)\n",
    "![Task1: optimal state-values and optimal policy](https://github.com/Thuang6413/ConestogaCollegeWorkplaceLevel2/blob/main/CSCN8020-25S-Sec1-RLP/Assignments/Problem3Task1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17aed74",
   "metadata": {},
   "source": [
    "### Task2: \n",
    "####  1. In-Place Value Iteration: Use a single array to store the state values. This means that you update the value of a state and immediately use that updated value in the subsequent updates.\n",
    "- updated the `value_iteration.py` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # For measuring optimization time\n",
    "\n",
    "################################################################################    \n",
    "###------- Stay same code as above, but with in-place value iteration -------###\n",
    "################################################################################\n",
    "\n",
    "def is_done(self, delta, theta_threshold):\n",
    "    return delta <= theta_threshold  # Theta threshold for convergence\n",
    "\n",
    "################################################################################    \n",
    "###------- Stay same code as above, but with in-place value iteration -------###\n",
    "################################################################################\n",
    "\n",
    "# Perform In-Place Value Iteration\n",
    "gridworld = GridWorld(ENV_SIZE)\n",
    "num_iterations = 1000\n",
    "theta_threshold = 0.0001 # As a standard convergence threshold in reinforcement\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "for iter in range(num_iterations):\n",
    "    delta = 0\n",
    "    for i in range(ENV_SIZE):\n",
    "        for j in range(ENV_SIZE):\n",
    "            if not gridworld.is_terminal_state(i, j):\n",
    "                v_old = gridworld.V[i, j]\n",
    "                gridworld.V[i, j], _, _ = gridworld.calculate_max_value(i, j)\n",
    "                delta = max(delta, abs(v_old - gridworld.V[i, j]))\n",
    "    if gridworld.is_done(delta, theta_threshold):\n",
    "        break\n",
    "runtime = time.time() - start_time  # End timing\n",
    "\n",
    "# Print results\n",
    "print(\"In-Place Value Iteration\")\n",
    "print(f\"Optimization Time: {runtime:.6f} seconds\")\n",
    "print(f\"Number of Iterations: {iter + 1}\")\n",
    "print(\"\\nOptimal Value Function (after %d iterations):\" % (iter + 1))\n",
    "for i in range(ENV_SIZE):\n",
    "    row = [f\"{gridworld.V[i, j]:7.2f}\" for j in range(ENV_SIZE)]\n",
    "    print(\" \".join(row))\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "gridworld.update_greedy_policy()\n",
    "gridworld.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16e49d",
   "metadata": {},
   "source": [
    "![Task2: optimal state-values and optimal policy](https://github.com/Thuang6413/ConestogaCollegeWorkplaceLevel2/blob/main/CSCN8020-25S-Sec1-RLP/Assignments/Problem3Task2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c4773",
   "metadata": {},
   "source": [
    "### Comments on Complexity\n",
    "- **Time**: Both have identical asymptotic time complexity. In-Place is significantly faster in practice due to:\n",
    "  - Fewer iterations (9 vs. 1000).\n",
    "  - No array copying, reduces memory.\n",
    "- **Duplicate Actions**: The action set `[\"Right\", \"Down\", \"Down\", \"Up\"]` includes redundant “Down” actions, increasing computation slightly. This affects both algorithms equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4e8c7",
   "metadata": {},
   "source": [
    "## Problem 4 [35]\n",
    "### Task\n",
    "Implement the off-policy Monte Carlo with Importance sampling algorithm to estimate the value function for the given gridworld. Use a fixed behavior policy b(a|s) (e.g., a random policy) to generate episodes and a greedy target policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7cdf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import numpy as np\n",
    "import time  # For measuring optimization time\n",
    "\n",
    "ENV_SIZE = 5\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((env_size, env_size))\n",
    "        # For importance sampling weights\n",
    "        self.C = np.zeros((env_size, env_size))\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0  # Terminal state value\n",
    "\n",
    "        # Define actions and rewards\n",
    "        self.actions = [(0, 1), (1, 0), (1, 0), (-1, 0)]  # Right, Down, Down, Up\n",
    "        self.action_description = [\"Right\", \"Down\", \"Down\", \"Up\"]\n",
    "        self.gamma = 0.9  # Discount factor (Problem 4)\n",
    "\n",
    "        self.rewards = {\n",
    "            (4, 4): 10,  # Terminal state\n",
    "            (2, 2): -5,  # Grey states\n",
    "            (3, 0): -5,\n",
    "            (0, 4): -5\n",
    "        }\n",
    "        self.default_reward = -1  # Regular states\n",
    "\n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "        self.pi_str = [[\"\" for _ in range(env_size)] for _ in range(env_size)]\n",
    "\n",
    "    '''@brief Returns the reward for a given state\n",
    "    '''\n",
    "    def get_reward(self, i, j):\n",
    "        return self.rewards.get((i, j), self.default_reward)\n",
    "\n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self.terminal_state\n",
    "\n",
    "    '''@brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(\n",
    "                \" \".join([f\"{action:7}\" if action else \"term   \" for action in row]))\n",
    "\n",
    "    '''@brief Calculate the maximum value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        best_actions = []\n",
    "        for action_index in range(len(self.actions)):\n",
    "            next_i, next_j = self.step(action_index, i, j)\n",
    "            if self.is_valid_state(next_i, next_j):\n",
    "                reward = self.get_reward(i, j)\n",
    "                value = reward + self.gamma * self.V[next_i, next_j]\n",
    "                if value > max_value - 1e-10:  # Handle numerical precision\n",
    "                    if value > max_value + 1e-10:\n",
    "                        max_value = value\n",
    "                        best_actions = [action_index]\n",
    "                        best_actions_str = self.action_description[action_index]\n",
    "                    else:\n",
    "                        best_actions.append(action_index)\n",
    "                        best_actions_str += \"|\" + \\\n",
    "                            self.action_description[action_index]\n",
    "        if best_actions:\n",
    "            best_action = best_actions[0]  # Choose first optimal action\n",
    "        return max_value, best_action, best_actions_str, best_actions\n",
    "\n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j  # Stay if invalid\n",
    "        return next_i, next_j\n",
    "\n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "\n",
    "    '''Return probability of each action under random behavior policy.\n",
    "    '''\n",
    "    def behavior_policy(self):\n",
    "        return 1.0 / len(self.actions)  # Uniform: 1/4 = 0.25\n",
    "\n",
    "    '''Return π(a|s) for action at state (i,j).\n",
    "    '''\n",
    "    def target_policy_prob(self, action_index, i, j):\n",
    "        _, _, _, best_actions = self.calculate_max_value(i, j)\n",
    "        if action_index in best_actions:\n",
    "            # Split probability among optimal actions\n",
    "            return 1.0 / len(best_actions)\n",
    "        return 0.0\n",
    "\n",
    "    '''Generate an episode using behavior policy.\n",
    "    '''\n",
    "    def generate_episode(self):\n",
    "        episode = []\n",
    "        i, j = 0, 0  # Start at (0,0)\n",
    "        while not self.is_terminal_state(i, j):\n",
    "            action_index = np.random.choice(len(self.actions))  # Random action\n",
    "            next_i, next_j = self.step(action_index, i, j)\n",
    "            reward = self.get_reward(i, j)  # Reward for current state\n",
    "            episode.append((i, j, action_index, reward))\n",
    "            i, j = next_i, next_j\n",
    "        # Add terminal state with its reward\n",
    "        terminal_reward = self.get_reward(i, j)\n",
    "        episode.append((i, j, None, terminal_reward))\n",
    "        return episode\n",
    "\n",
    "    '''Update greedy policy based on current value function.\n",
    "    '''\n",
    "    def update_greedy_policy(self):\n",
    "        for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if self.is_terminal_state(i, j):\n",
    "                    self.pi_greedy[i, j] = 0\n",
    "                    self.pi_str[i][j] = \"\"\n",
    "                    continue\n",
    "                _, self.pi_greedy[i, j], action_str, _ = self.calculate_max_value(\n",
    "                    i, j)\n",
    "                self.pi_str[i][j] = action_str\n",
    "\n",
    "    '''Run off-policy Monte Carlo with importance sampling.\n",
    "    '''\n",
    "    def monte_carlo_off_policy(self, num_episodes):\n",
    "        for episode_idx in range(num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "            G = 0  # Return\n",
    "            W = 1.0  # Importance sampling ratio\n",
    "            # Process episode backward\n",
    "            for t in range(len(episode) - 1, -1, -1):  # Include terminal state\n",
    "                i, j, action_index, reward = episode[t]\n",
    "                G = reward + self.gamma * G  # Update return\n",
    "                if t < len(episode) - 1:  # Skip ratio for terminal action\n",
    "                    W *= self.target_policy_prob(action_index,\n",
    "                                                 i, j) / self.behavior_policy()\n",
    "                self.C[i, j] += W\n",
    "                if self.C[i, j] > 0:\n",
    "                    self.V[i, j] += (W / self.C[i, j]) * (G - self.V[i, j])\n",
    "\n",
    "\n",
    "# Run Off-policy Monte Carlo\n",
    "gridworld = GridWorld(ENV_SIZE)\n",
    "num_episodes = 10000\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "gridworld.monte_carlo_off_policy(num_episodes)\n",
    "runtime = time.time() - start_time  # End timing\n",
    "\n",
    "# Print results\n",
    "print(\"Off-policy Monte Carlo with Importance Sampling\")\n",
    "print(f\"Optimization Time: {runtime:.6f} seconds\")\n",
    "print(f\"Number of Episodes: {num_episodes}\")\n",
    "print(\"\\nEstimated Value Function:\")\n",
    "for i in range(ENV_SIZE):\n",
    "    row = [f\"{gridworld.V[i, j]:7.2f}\" for j in range(ENV_SIZE)]\n",
    "    print(\" \".join(row))\n",
    "\n",
    "print(\"\\nGreedy Policy:\")\n",
    "gridworld.update_greedy_policy()\n",
    "gridworld.print_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a03296",
   "metadata": {},
   "source": [
    "![Problem4: optimal state-values and optimal policy](https://github.com/Thuang6413/ConestogaCollegeWorkplaceLevel2/blob/main/CSCN8020-25S-Sec1-RLP/Assignments/Problem4Task.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6bf9fd",
   "metadata": {},
   "source": [
    "### Detailed Comparison\n",
    "- **Optimization Time:** In-Place Value Iteration is significantly faster than Off-policy Monte Carlo. This significant difference arises because Value Iteration performs deterministic updates over all 9 iterations, while Monte Carlo processes 10,000 episodes.\n",
    "- **Complexity:** Monte Carlo’s scales with the number of episodes and step length, making it less efficient for this small MDP but adaptable to unknown environments.\n",
    "- **Model Dependence:** In-Place Value Iteration requires a known model. However, Monte Carlo is Model-free that relys on sampled episodes, suitable for scenarios where the model is unavailable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
